
# 内存虚拟化

内存虚拟化的目标是提供一种方法,使得每个虚拟机都能够访问自己独立的虚拟内存空间,而这些虚拟内存空间背后实际上是由宿主机的物理内存来提供的.在虚拟化环境中,这种管理机制要有效地确保虚拟机之间的内存隔离,同时优化资源使用率.

## 虚拟机内存地址转换

我们先来快速回顾一下操作系统中的地址转换过程, 这对理解虚拟机内存地址转换非常重要

![20240321222310](https://raw.githubusercontent.com/learner-lu/picbed/master/20240321222310.png)

一个标准 table walk 如上图所示, CR3 寄存器保存当前进程的根页表地址, linux 中采用 48 位虚拟地址, 4KB 的页面, 4 级页表, 使用虚拟地址[47:39],索引L1级页表,得到L2级页表的页基地址.使用[38:30]索引L2级页表,得到L2级页表的页基址, 最终在L4级页表得到实际物理页面地址,加上页内偏移,可以得到物理地址

> 详细的内容参见 [虚拟地址转换](https://luzhixing12345.github.io/klinux/articles/mm/%E8%99%9A%E6%8B%9F%E5%9C%B0%E5%9D%80%E8%BD%AC%E6%8D%A2/), 这里不再赘述

在Linux这种使用虚拟地址的OS中,虚拟地址经过page table转换可得到物理地址, 如下图所示

![20240321234343](https://raw.githubusercontent.com/learner-lu/picbed/master/20240321234343.png)

但是如果这个操作系统是运行在虚拟机上的, 客户端转换得到的物理地址依然只是一个中间的物理地址, 需要再经过 VMM/hypervisor 的转换,才能得到最终的物理地址(Host Phyical Address - HPA). 因此还需要一次地址转换, 如下图所示

![20240321234716](https://raw.githubusercontent.com/learner-lu/picbed/master/20240321234716.png)

因此相比之下, 普通的进程只需要一次地址转换 VA->PA, 但是在虚拟化环境中需要经过三次地址转换

1. 首先通过 Guest 的页表将 Guest Virtual Address (GVA)转换为 Guest Physical Address(GPA)
2. GPA 在 Qemu 的实现当中实际上是对应映射到 Host 中一大块 mmap 的内存上的,所以我们还需要将 GPA 再转换为 Host Virtual Address(HVA)
3. 最后再通过 Host 上的页表将 HVA 转化为 Host Physical Address(HPA)

![20241108161015](https://raw.githubusercontent.com/learner-lu/picbed/master/20241108161015.png)

这一整套流程非常繁重,从而使得虚拟机中内存访问的性能极为低下, 并且传统的IA32架构从硬件上只支持一次地址转换,即由CR3寄存器指向进程第一级页表的首地址,通过MMU查询进程的各级页表,获得物理地址.

因此为了完成这种二级地址转换, 现代普遍采用两种优化方式, 分别是**软件页表虚拟化(shadow page)和硬件页表虚拟化(extended page)**

> [!TIP]
> 下文中会出现一些字母的缩写代称
> - H 指主机 host
> - G 指客户端 guest
> - PA 指物理地址 physical address
> - VA 指虚拟地址 virtual address
> - PT 指页表 page table
>
> 例如对于 Guest OS 内部的进程页表我们称为 gPT(guest Page Table)

## shadow page(软件)

在早期的时候 Intel 硬件对虚拟化并没有很好的支持,因此 Hypervisor 只能先在软件层面进行优化, **影子页表**(Shadow Page Table)应运而生.

影子页表的思路非常简单, 既然虚拟化中的多次地址转换开销很大, 那么在 vmm 中直接记录一张**映射表**, 也称为影子页表 **sPT(shadow Page Table)**, 这张表记录 gva 到 hpa 的关系, 如下图所示

![20250226110558](https://raw.githubusercontent.com/learner-lu/picbed/master/20250226110558.png)

> [!TIP]
> 图中画出的 spt 是一个映射表, 但其实它就是 qemu 管理的一块内存区域, 也是以页表的形式保存管理

以 Intel 为例,由于读写 CR3 寄存器(存放页顶级表指针)的操作是敏感指令,我们的 vmm 可以很轻易地截获这个操作, 并将**页表替换为存放 GVA→HPA 映射关系的影子页表**,这样就能直接完成由 GVA 到 HPA 的转换过程. 现在翻译虚拟地址时只需要通过 gva 查 spt 就可以直接得到最终的 hpa 地址, **原先的三次地址翻译直接被简化到了一次!**

> [!IMPORTANT]
> 部分读者可能会感到疑惑, 这个 spt 是如何构建的? spt 需要记录 gva 到 hpa 的映射, 但是按理来说qemu作为一个系统进程应该不知道 hva 到 hpa 的映射的?
> 
> 是的, qemu 作为用户态进程确实无法直接访问 hva->hpa 的映射, 因为本身这部分是操作系统负责的, 但它通过与内核(如 KVM)的协作来实现这一功能, kvm 作为一个内核模块会将这部分信息传递给 qemu (有对应的 kvm api 函数)

这种纯软件的方法虽然能够解决问题, 三次转换变一次当然很好, 那么代价是什么呢?

- 首先我们注意到 spt 记录的是 gva 的映射, 也就是说 vmm 需要为每个guest VM中的**每个进程的gPT都维护一个对应的sPT**, 且不论多虚拟机, 为每一个进程都记录一份 spt 这显然大大增加了内存的开销.
- 其次每个进程的页表是不断变化的, 分配/释放内存时都会修改页表, 那么此时 spt 也要跟着同步. qemu 的做法是
  1. 首先 qemu 会将gPT本身使用的物理页面设为**write protected**
  2. 每当gPT有变动的时候(比如添加或删除了一个页表项),就会产生被VMM截获的**page fault异常**
  3. 重新计算GVA->HPA的映射,更改sPT中对应的页表项

我们需要为 Guest VM 中的每套页表都独立维护一份影子页表,且需要多次在 VMM 与 VM 间进行切换,这有着不小的开销. 在一些场景下,这种影子页表机制造成的开销可以占到整个VMM软件负载的75%.

## extended page(硬件)

从软件层面似乎已经是难以有更好的优化的方案了,因此硬件层面的对内存虚拟化的支持便应运而生. 各大CPU厂商相继推出了硬件辅助的内存虚拟化技术,比如Intel的**EPT**(Extended Page Table)和AMD的NPT(**Nested Page Table**),它们都能够从硬件上同时支持 GVA->GPA->HPA 的两级地址转换的技术.

> 硬件虚拟化根据 Intel/AMD 的硬件技术有两个名字, 下文介绍 Intel x86 处理器支持的 EPT 

整个流程如下图所示

![20240322111926](https://raw.githubusercontent.com/learner-lu/picbed/master/20240322111926.png)

首先先看上面一行的绿色虚线, 它代表的是 guest OS 内部完成的 GVA -> GPA 的地址转换过程. guest 使用 gCR3 记录每个进程的页表基地址(物理地址)

虽然 GVA -> GPA 的页表逐级遍历中会拿到下一级新的 gPA, 到这里虽然已经转换成物理地址,但是由于是客户机物理地址,不等同于宿主机的物理地址,所以并不能直接访问, gPA 都需要经过VMM的翻译才能得到真实的HPA, 需要借助于第二次的转换,也就是EPT的转换

**每个guest VM有一个由VMM维护的EPT**, 它维护 GPA -> HPA 的映射.

> 其实,EPT/NPT就是一种扩展的MMU(以下称EPT/NPT MMU),它可以交叉地查找gPT和EPT两个页表

首先它会查找guest VM中CR3寄存器(gCR3)指向的PML4页表,由于gCR3中存储的地址是GPA,因此CPU需要查找nPT来获取gCR3的GPA对应的HPA, 我们称一次nPT的查找过程为一次nested walk. 如果找到了,也就是获得了一级页表的物理首地址后,就可以用对应的索引得到二级页表的GVA.接下来又是通过一次nested walk得到下一级页表的首地址,然后重复上述过程,最终获得该GVA对应的HPA, 

不同于影子页表是一个进程需要一个sPT,EPT/NPT MMU解耦了GVA->GPA转换和GPA->HPA转换之间的依赖关系,**一个VM只需要一个nPT**,减少了内存开销.如果guest VM中发生了page fault,可直接由guest OS处理,**不会产生vm-exit,减少了CPU的开销**.可以说,EPT/NPT MMU这种硬件辅助的内存虚拟化技术解决了纯软件实现存在的两个问题.

因为每次都需要对页基地址进行翻译,所以如果查询guest页表结构为n级,host页表结构为m级,那么翻译页表的gPA就需要n * m次 ,又因为最终获得的gPA还需要通过host页表进行查询,因此最后又需要m次,总计需要 n + nm + m 次访存, 整个过程完全由硬件实现:

- n(guest page walk)
- n * m(翻译所有的页表对应的sPA)
- m (最后一轮翻译gPA)

> 对于 4 级页表来说就是 24 次

## 参考

- [vmware VMware_paravirtualization.pdf](https://www.vmware.com/content/dam/digitalmarketing/vmware/en/pdf/techpaper/VMware_paravirtualization.pdf)
- [vmware Perf_ESX_Intel-EPT-eval.pdf](https://www.vmware.com/pdf/Perf_ESX_Intel-EPT-eval.pdf)
- [memory virtualization: shadow page & nest page](https://blog.csdn.net/hit_shaoqi/article/details/121887459)
- [内存虚拟化-shadow实现](https://blog.csdn.net/hx_op/article/details/103980411)
- [Introduce_to_virtualization 内存虚拟化](https://github.com/0voice/Introduce_to_virtualization/blob/main/virtualization_type/memory_virtualization/%E5%86%85%E5%AD%98%E8%99%9A%E6%8B%9F%E5%8C%96.md)
- [QEMU内存分析(四):ept页表构建](https://www.cnblogs.com/edver/p/14662609.html)